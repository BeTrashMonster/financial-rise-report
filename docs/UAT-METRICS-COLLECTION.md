# UAT Metrics Collection System - Financial RISE

**Version:** 1.0
**Date:** 2025-12-22
**UAT Period:** 4 Weeks

## Table of Contents

1. [Metrics Overview](#metrics-overview)
2. [Survey Instruments](#survey-instruments)
3. [Analytics Tracking](#analytics-tracking)
4. [Data Collection Methods](#data-collection-methods)
5. [Reporting Dashboards](#reporting-dashboards)
6. [Analysis Framework](#analysis-framework)

---

## Metrics Overview

### Metric Categories

**1. System Performance Metrics**
- Page load times
- API response times
- Report generation times
- Error rates
- Uptime/availability

**2. User Engagement Metrics**
- Assessment completion rate
- Average completion time
- Feature adoption rate
- Daily/weekly active users
- Session duration

**3. Quality Metrics**
- Bug count by severity
- Bug fix time
- Test case pass rate
- DISC profile accuracy
- Phase determination accuracy

**4. User Satisfaction Metrics**
- System Usability Scale (SUS)
- Net Promoter Score (NPS)
- Feature satisfaction ratings
- Overall satisfaction
- Likelihood to recommend

**5. Business Metrics**
- Assessments created
- Assessments completed
- Reports generated
- User retention
- Client satisfaction

---

## Survey Instruments

### 1. Week 1 Training Effectiveness Survey

**Send:** Thursday Week 1, 5:00 PM
**Response Time:** 24 hours
**Tool:** Google Forms

**Questions:**

**Training Quality (1-5 scale: Poor to Excellent)**
1. How would you rate the kickoff session?
2. How would you rate the "Creating Assessments" training?
3. How would you rate the "Conducting Assessments" training?
4. How would you rate the "Reports & Interpretation" training?

**Learning Outcomes (1-5 scale: Strongly Disagree to Strongly Agree)**
5. I understand how to create an assessment.
6. I understand how to conduct an assessment.
7. I understand how to generate and interpret reports.
8. I feel confident using the platform with real clients.

**Open-Ended Questions**
9. What aspect of training was most helpful?
10. What additional training would you like?
11. Were there any confusing or unclear parts?

**Overall**
12. Overall training satisfaction (1-5)
13. Any additional comments or suggestions?

---

### 2. Week 1 First Impressions Survey

**Send:** Friday Week 1, 5:00 PM
**Response Time:** 24 hours
**Tool:** Google Forms

**Questions:**

**First Impressions (1-5 scale)**
1. The platform is visually appealing.
2. Navigation is intuitive and easy to understand.
3. Features are where I expected them to be.
4. The dashboard provides useful information.
5. Help documentation is easy to find and helpful.

**Feature Discovery**
6. Which features did you explore this week? (Multi-select)
   - Creating assessments
   - Viewing assessment list
   - Generating reports
   - User profile settings
   - Help documentation
   - Other: _____

**Ease of Use (1-5 scale)**
7. How easy was it to create your first assessment?
8. How easy was it to navigate the sample assessment?
9. How easy was it to generate a report?

**Open-Ended**
10. What was your favorite feature or aspect?
11. What was most confusing or frustrating?
12. What would you change or improve?

---

### 3. Week 2 Real Client Testing Survey

**Send:** Friday Week 2, 5:00 PM
**Response Time:** 48 hours
**Tool:** Typeform (engaging format)

**Questions:**

**Client Testing Experience**
1. How many clients did you test with this week? (Number)
2. Which assessment type did you use most? (Self-administered / Collaborative / Both)
3. Did clients complete assessments successfully? (Yes / Yes with issues / No)
4. If issues, please describe: (Open text)

**Assessment Workflow (1-5 scale)**
5. Creating assessments was smooth and efficient.
6. Monitoring client progress was easy.
7. Adding consultant notes worked well.
8. Email invitations were delivered reliably.

**Report Quality (1-5 scale)**
9. Reports generated successfully every time.
10. DISC profiles seemed accurate for my clients.
11. Phase determinations made sense.
12. Recommendations were relevant and actionable.
13. Report formatting was professional.

**Client Feedback**
14. What did clients say about the assessment experience? (Open text)
15. Would your clients recommend this to others? (Yes / Maybe / No / Didn't ask)

**Issues Encountered**
16. Did you encounter any bugs or technical issues? (Yes / No)
17. If yes, did you report them? (Yes / No / Not sure)
18. Were issues resolved quickly? (Yes / Still pending / No)

**Overall**
19. Overall satisfaction with Week 2 (1-5)
20. Additional comments: (Open text)

---

### 4. Week 2 Net Promoter Score (NPS) Survey

**Send:** Friday Week 2, 5:30 PM
**Response Time:** 48 hours
**Tool:** Simple email or Typeform

**Questions:**

1. **On a scale of 0-10, how likely are you to recommend Financial RISE to a colleague?**
   - 0 = Not at all likely
   - 10 = Extremely likely

2. **What is the primary reason for your score?** (Open text)

3. **What would make you more likely to recommend Financial RISE?** (Open text)

**NPS Calculation:**
- Promoters (9-10): % who gave 9-10
- Passives (7-8): % who gave 7-8
- Detractors (0-6): % who gave 0-6
- NPS Score = % Promoters - % Detractors

---

### 5. Week 3 System Usability Scale (SUS) Survey

**Send:** Friday Week 3, 5:00 PM
**Response Time:** 48 hours
**Tool:** Google Forms

**Standard SUS Questions (1-5 scale: Strongly Disagree to Strongly Agree)**

1. I think that I would like to use this system frequently.
2. I found the system unnecessarily complex.
3. I thought the system was easy to use.
4. I think that I would need the support of a technical person to be able to use this system.
5. I found the various functions in this system were well integrated.
6. I thought there was too much inconsistency in this system.
7. I would imagine that most people would learn to use this system very quickly.
8. I found the system very cumbersome to use.
9. I felt very confident using the system.
10. I needed to learn a lot of things before I could get going with this system.

**SUS Scoring:**
- Questions 1, 3, 5, 7, 9: Score = (response - 1)
- Questions 2, 4, 6, 8, 10: Score = (5 - response)
- Sum all scores and multiply by 2.5
- Final SUS score: 0-100

**Interpretation:**
- 80+: Excellent (Grade A)
- 68-79: Good (Grade B)
- 51-67: OK (Grade C)
- <51: Poor (Grade F)

---

### 6. Week 3 Feature Satisfaction Survey

**Send:** Friday Week 3, 5:30 PM
**Response Time:** 48 hours
**Tool:** Typeform

**Feature Ratings (1-5 scale: Very Dissatisfied to Very Satisfied)**

**Core Features**
1. Assessment creation
2. Self-administered assessments
3. Collaborative assessments
4. Progress monitoring
5. Consultant notes
6. Report generation
7. DISC profiling
8. Phase determination
9. Report customization
10. PDF downloads

**Supporting Features**
11. Dashboard
12. Assessment list/filtering
13. Search functionality
14. User profile settings
15. Help documentation
16. Email notifications

**For Each Low Rating (1-2), Ask:**
- What specifically was unsatisfactory about [feature]?

**Most/Least Used Features**
17. Which feature do you use most often?
18. Which feature do you use least often?
19. Are there missing features you need?

---

### 7. Week 4 Comprehensive Final Survey

**Send:** Wednesday Week 4, 9:00 AM
**Response Time:** 48 hours
**Tool:** SurveyMonkey (for advanced analytics)

**Overall Experience (1-5 scale)**
1. Overall satisfaction with Financial RISE
2. Likelihood to continue using after UAT
3. Likelihood to recommend to colleagues
4. Platform meets my professional needs
5. Platform improves my consulting practice

**Workflow Assessment (1-5 scale)**
6. Creating assessments is efficient
7. Conducting assessments is streamlined
8. Generating reports is fast and easy
9. Client experience is positive
10. Results are accurate and useful

**Value Assessment**
11. What is the biggest benefit of Financial RISE? (Open text)
12. What problem does it solve for you? (Open text)
13. How has it improved your consulting practice? (Open text)

**DISC Integration**
14. DISC profiling adds value to assessments (1-5)
15. DISC-adapted reports are helpful (1-5)
16. I use DISC insights when presenting to clients (Always / Sometimes / Rarely / Never)

**Improvements**
17. What is the one thing you would change? (Open text)
18. What feature would you add? (Open text)
19. What should we prioritize post-launch? (Open text)

**Launch Readiness**
20. Is the platform ready for production launch? (Yes / Yes with minor fixes / No / Unsure)
21. If no/unsure, what needs to be addressed? (Open text)

**Testimonial Request**
22. Would you be willing to provide a testimonial? (Yes / Maybe / No)
23. Would you be willing to record a video testimonial? (Yes / Maybe / No)

---

### 8. Client Feedback Survey (Sent by Consultants)

**Send:** After client completes assessment
**Tool:** Simple Google Form (provided to consultants)

**Questions:**

1. How would you rate the assessment experience? (1-5 stars)
2. Was the assessment easy to understand? (Yes / No / Somewhat)
3. Did you experience any technical issues? (Yes / No)
4. If yes, please describe: (Open text)
5. How long did the assessment take? (< 30 min / 30-45 min / 45-60 min / > 60 min)
6. Would you recommend this assessment to other business owners? (Yes / No / Maybe)
7. Additional comments: (Open text)

---

## Analytics Tracking

### Google Analytics 4 (GA4) Setup

**Custom Events:**

```javascript
// Assessment Created
gtag('event', 'assessment_created', {
  'event_category': 'Assessment',
  'user_id': userId,
  'timestamp': Date.now()
});

// Assessment Started (by client)
gtag('event', 'assessment_started', {
  'event_category': 'Assessment',
  'assessment_id': assessmentId,
  'assessment_type': 'self-administered|collaborative'
});

// Assessment Completed
gtag('event', 'assessment_completed', {
  'event_category': 'Assessment',
  'assessment_id': assessmentId,
  'completion_time': completionTimeSeconds,
  'question_count': 25
});

// Report Generated
gtag('event', 'report_generated', {
  'event_category': 'Report',
  'report_type': 'consultant|client',
  'generation_time': generationTimeSeconds,
  'disc_profile': discProfile,
  'phase': primaryPhase
});

// Report Downloaded
gtag('event', 'report_downloaded', {
  'event_category': 'Report',
  'report_type': 'consultant|client',
  'file_format': 'pdf'
});

// Error Encountered
gtag('event', 'error_occurred', {
  'event_category': 'Error',
  'error_type': errorType,
  'error_message': errorMessage,
  'page': currentPage
});

// Feature Used
gtag('event', 'feature_used', {
  'event_category': 'Feature',
  'feature_name': featureName,
  'user_id': userId
});
```

**Custom Metrics:**

| Metric | Type | Description |
|--------|------|-------------|
| assessment_completion_rate | Percentage | % of started assessments completed |
| avg_completion_time | Time (seconds) | Average time to complete assessment |
| report_generation_time | Time (seconds) | Time to generate report |
| error_rate | Percentage | % of sessions with errors |
| feature_adoption | Count | # users using each feature |

**User Properties:**

```javascript
gtag('set', 'user_properties', {
  'user_role': 'consultant',
  'account_created': '2025-12-22',
  'uat_participant': true
});
```

---

### Application Performance Monitoring (APM)

**CloudWatch Custom Metrics:**

**Backend Metrics:**
```javascript
// API Response Time
cloudwatch.putMetricData({
  MetricName: 'APIResponseTime',
  Value: responseTime,
  Unit: 'Milliseconds',
  Dimensions: [
    { Name: 'Endpoint', Value: '/api/assessments' },
    { Name: 'Method', Value: 'GET' }
  ]
});

// Database Query Time
cloudwatch.putMetricData({
  MetricName: 'DatabaseQueryTime',
  Value: queryTime,
  Unit: 'Milliseconds',
  Dimensions: [
    { Name: 'Query', Value: 'getAssessments' }
  ]
});

// PDF Generation Time
cloudwatch.putMetricData({
  MetricName: 'PDFGenerationTime',
  Value: generationTime,
  Unit: 'Seconds',
  Dimensions: [
    { Name: 'ReportType', Value: 'consultant' }
  ]
});
```

**Frontend Metrics:**
```javascript
// Page Load Time (using Navigation Timing API)
const pageLoadTime = performance.timing.loadEventEnd - performance.timing.navigationStart;

// Send to analytics
analytics.track('page_load_time', {
  page: window.location.pathname,
  loadTime: pageLoadTime
});
```

---

## Data Collection Methods

### 1. Automated Collection

**System Logs:**
- Application logs (errors, warnings, info)
- Access logs (page views, API calls)
- Performance logs (response times, query times)

**Analytics:**
- Google Analytics events
- Custom event tracking
- User behavior flows

**Database:**
- Assessment metadata
- User activity records
- Performance metrics

### 2. Manual Collection

**Bug Reports:**
- Slack #uat-support channel
- Email to uat-bugs@financialrise.com
- Bug tracking system (Jira/Linear)

**Feedback:**
- Slack conversations
- 1-on-1 interviews
- Group call discussions

**Observations:**
- Screen recordings (with consent)
- Usability testing sessions
- Think-aloud protocols

### 3. Survey Collection

**Distribution:**
- Email with survey link
- Slack announcements
- In-app notifications

**Reminders:**
- 24 hours before deadline
- Day of deadline

**Incentives:**
- Recognition in final report
- Early access to new features
- Thank-you gift cards (optional)

---

## Reporting Dashboards

### Dashboard 1: UAT Overview

**Metrics Displayed:**
- Total participants: 10
- Active participants this week: 8
- Assessments created: 45
- Assessments completed: 38
- Reports generated: 76
- Bugs reported: 12 (2 critical, 3 high, 5 medium, 2 low)
- Avg SUS score: 82 (Grade A)
- NPS score: +55

**Refresh Rate:** Real-time (WebSocket updates)

---

### Dashboard 2: Performance Metrics

**Charts:**
1. **Page Load Times** (Line chart, last 7 days)
   - Homepage
   - Dashboard
   - Assessment page
   - Reports page

2. **API Response Times** (Line chart, last 7 days)
   - GET /assessments
   - POST /assessments
   - POST /reports/generate

3. **Report Generation Times** (Histogram)
   - Distribution of generation times
   - Target: <5 seconds

4. **Error Rate** (Line chart)
   - Daily error rate %
   - Error types breakdown

---

### Dashboard 3: User Engagement

**Charts:**
1. **Daily Active Users** (Bar chart)
2. **Assessments Created vs Completed** (Stacked bar)
3. **Feature Adoption** (Horizontal bar)
   - % of users who used each feature
4. **Session Duration** (Histogram)

---

### Dashboard 4: Survey Results

**Metrics:**
- Response rate: 90%
- Avg overall satisfaction: 4.2/5
- SUS score: 82
- NPS: +55

**Charts:**
1. **Feature Satisfaction** (Radar chart)
2. **Training Effectiveness** (Bar chart)
3. **NPS Distribution** (Pie chart: Promoters, Passives, Detractors)

---

## Analysis Framework

### Weekly Analysis Cycle

**Monday:**
- Review previous week's data
- Identify trends and patterns
- Flag critical issues

**Wednesday:**
- Mid-week check-in
- Update dashboards
- Communicate urgent findings

**Friday:**
- Compile weekly report
- Present to stakeholders
- Plan next week priorities

---

### Analysis Methods

**Quantitative Analysis:**
- Descriptive statistics (mean, median, mode)
- Trend analysis (week-over-week changes)
- Correlation analysis (feature usage vs satisfaction)
- Performance benchmarking (vs targets)

**Qualitative Analysis:**
- Thematic coding of open-ended responses
- Sentiment analysis
- User journey mapping
- Pain point identification

**Combined Analysis:**
- Triangulation (compare surveys, analytics, observations)
- Root cause analysis (5 Whys)
- Impact assessment (severity x frequency)

---

### Weekly Report Template

```markdown
# UAT Week [N] Summary Report

**Date:** [Week Start] - [Week End]
**Participants:** [N active / N total]
**Report Author:** [Name]

## Executive Summary
[2-3 sentences highlighting key findings]

## Key Metrics
| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| Assessments Completed | - | - | - |
| SUS Score | >80 | - | - |
| Bug Count (Critical) | 0 | - | - |

## Accomplishments
- [Key accomplishment 1]
- [Key accomplishment 2]

## Issues & Resolutions
1. **[Issue]**
   - Severity: [Critical/High/Medium/Low]
   - Status: [Resolved/In Progress/Pending]
   - Resolution: [Description]

## User Feedback Highlights
**Positive:**
- "[Quote from participant]"
- "[Quote from participant]"

**Needs Improvement:**
- "[Feedback]"
- "[Feedback]"

## Next Week Focus
- [Priority 1]
- [Priority 2]

## Attachments
- Survey results
- Bug list
- Performance charts
```

---

**Metrics Collection System Version:** 1.0
**Owner:** Project Manager + QA Lead
**Last Updated:** 2025-12-22
